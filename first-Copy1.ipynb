{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de92cc55",
   "metadata": {
    "id": "de92cc55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chira\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# ðŸ“¦ Imports\n",
    "# ------------------------------\n",
    "import os, glob, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import welch\n",
    "\n",
    "import neurokit2 as nk\n",
    "import antropy as ant\n",
    "import traceback\n",
    "\n",
    "# Hugging Face dataset loader (for Dreaddit)\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TensorFlow for FER-2013 images\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d677a3b-5e7d-42b9-b75b-037029e357a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ ecg_model.pkl not found, will retrain ecg model.\n",
      "âš ï¸ text_model.pkl not found, will retrain text model.\n",
      "âš ï¸ face_model.pkl not found, will retrain face model.\n",
      "âš ï¸ voice_model.pkl not found, will retrain voice model.\n",
      "âš ï¸ fusion_inputs.pkl not found, will retrain fusion model.\n",
      "âš ï¸ No models found â€” all will be trained fresh.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸš€ AUTO-LOAD PRETRAINED MODELS AND PROBABILITIES (Restart-Safe)\n",
    "# ============================================================\n",
    "\n",
    "import os, pickle\n",
    "\n",
    "model_files = {\n",
    "    \"ecg\": \"ecg_model.pkl\",\n",
    "    \"text\": \"text_model.pkl\",\n",
    "    \"face\": \"face_model.pkl\",\n",
    "    \"voice\": \"voice_model.pkl\",\n",
    "    \"fusion\": \"fusion_inputs.pkl\"\n",
    "}\n",
    "\n",
    "loaded_ok = []\n",
    "for name, file in model_files.items():\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            data = pickle.load(open(file, \"rb\"))\n",
    "            # Handle both dict or model-only pickle structures\n",
    "            if isinstance(data, dict):\n",
    "                globals().update(data)\n",
    "            else:\n",
    "                globals()[f\"{name}_model\"] = data\n",
    "            loaded_ok.append(name)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error loading {name} model â†’ {e}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ {file} not found, will retrain {name} model.\")\n",
    "\n",
    "if loaded_ok:\n",
    "    print(f\"âœ… Successfully loaded pretrained models: {', '.join(loaded_ok)}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No models found â€” all will be trained fresh.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f894eb0",
   "metadata": {
    "id": "7f894eb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 2 labels: (array([0, 1, 2, 3, 4, 6, 7], dtype=int32), array([2142701,  800800,  430500,  253400,  537599,   45500,   44800]))\n",
      "Subject 3 labels: (array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32), array([2345699,  798000,  448000,  262500,  546001,   51100,   46900,\n",
      "         46900]))\n",
      "Subject 4 labels: (array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32), array([2314199,  810601,  444500,  260400,  563500,   35699,   30800,\n",
      "         36401]))\n",
      "Subject 5 labels: (array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32), array([2142700,  838600,  451500,  261800,  555800,   50401,   30799,\n",
      "         49000]))\n",
      "Subject 6 labels: (array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32), array([2733499,  826000,  455000,  260400,  550900,   40600,   35001,\n",
      "         48300]))\n",
      "Subject 7 labels: (array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32), array([1472098,  830200,  448000,  260401,  553001,   35000,   30799,\n",
      "         37101]))\n",
      "Subject 8 labels: (array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32), array([1616300,  818300,  469000,  258999,  557200,   34300,   35701,\n",
      "         36400]))\n",
      "Subject 9 labels: (array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32), array([1435700,  826000,  451500,  260400,  555100,   42000,   43400,\n",
      "         42000]))\n",
      "Subject 10 labels: (array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32), array([1589000,  826000,  507500,  260400,  557200,   35700,   31500,\n",
      "         39900]))\n",
      "Subject 11 labels: (array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32), array([1443400,  826000,  476000,  257600,  553701,   35000,   36399,\n",
      "         35000]))\n",
      "Subject 13 labels: (array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32), array([1679300,  826001,  464800,  267400,  556499,   34300,   33600,\n",
      "         14000]))\n",
      "Subject 14 labels: (array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32), array([1648499,  826000,  472500,  260401,  555800,   44100,   38500,\n",
      "         37800]))\n",
      "Subject 15 labels: (array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32), array([1457401,  822500,  480200,  260400,  555799,   35000,   32900,\n",
      "         32200]))\n",
      "Subject 16 labels: (array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32), array([1717100,  826000,  471101,  257600,  554399,   38500,   39900,\n",
      "         37100]))\n",
      "Subject 17 labels: (array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32), array([1917301,  826700,  506100,  260400,  511700,   40600,   41299,\n",
      "         39900]))\n",
      "âœ… ECG dataset: (129500, 18)\n",
      "Class distribution: Counter({np.int64(0): 102250, np.int64(1): 27250})\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# ðŸ’¡ ECG Feature Extraction\n",
    "# ----------------------\n",
    "def extract_ecg_features(segment, fs=700):  # WESAD chest ECG = 700 Hz\n",
    "    if len(segment) == 0:\n",
    "        return None\n",
    "    try:\n",
    "        features = [\n",
    "            np.mean(segment), np.std(segment), np.min(segment), np.max(segment),\n",
    "            np.percentile(segment, 25), np.percentile(segment, 75),\n",
    "            np.median(segment), skew(segment), kurtosis(segment)\n",
    "        ]\n",
    "\n",
    "        # HRV\n",
    "        try:\n",
    "            _, rpeaks = nk.ecg_peaks(segment, sampling_rate=fs)\n",
    "            rr_intervals = np.diff(rpeaks[\"ECG_R_Peaks\"]) / fs\n",
    "            if len(rr_intervals) > 2:\n",
    "                features.extend([\n",
    "                    np.mean(rr_intervals), np.std(rr_intervals),\n",
    "                    np.sqrt(np.mean(np.square(np.diff(rr_intervals)))),\n",
    "                    np.sum(np.abs(np.diff(rr_intervals)) > 0.05) / len(rr_intervals)\n",
    "                ])\n",
    "            else: features.extend([0,0,0,0])\n",
    "        except: features.extend([0,0,0,0])\n",
    "\n",
    "        # Frequency\n",
    "        try:\n",
    "            if len(rr_intervals) > 4:\n",
    "                f, pxx = welch(rr_intervals, fs=4.0, nperseg=min(256, len(rr_intervals)))\n",
    "                vlf = np.trapz(pxx[(f >= 0.003) & (f < 0.04)])\n",
    "                lf = np.trapz(pxx[(f >= 0.04) & (f < 0.15)])\n",
    "                hf = np.trapz(pxx[(f >= 0.15) & (f < 0.4)])\n",
    "                lf_hf = lf / hf if hf > 0 else 0\n",
    "                features.extend([vlf, lf, hf, lf_hf])\n",
    "            else: features.extend([0,0,0,0])\n",
    "        except: features.extend([0,0,0,0])\n",
    "\n",
    "        # Entropy\n",
    "        try: features.append(ant.sample_entropy(segment))\n",
    "        except: features.append(0)\n",
    "\n",
    "        return features\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# ðŸ“¥ Load WESAD ECG Dataset (fixed label mapping)\n",
    "# ----------------------\n",
    "X_ecg, y_ecg = [], []\n",
    "window_size = 256\n",
    "data_path = r\"C:\\Users\\chira\\OneDrive\\Desktop\\ECG BASED STRESS AND EMOTION DETECTION\\WESAD\"\n",
    "\n",
    "for subject_id in range(2, 18):\n",
    "    folder = os.path.join(data_path, f\"S{subject_id}\")\n",
    "    pkl_files = glob.glob(os.path.join(folder, \"*.pkl\"))\n",
    "    if not pkl_files:\n",
    "        continue\n",
    "\n",
    "    with open(pkl_files[0], \"rb\") as f:\n",
    "        data = pickle.load(f, encoding=\"latin1\")\n",
    "        ecg = np.array(data['signal']['chest']['ECG'])[:, 0]\n",
    "        labels = np.array(data['label'])\n",
    "\n",
    "    # Debug: check label distribution per subject\n",
    "    print(f\"Subject {subject_id} labels:\", np.unique(labels, return_counts=True))\n",
    "\n",
    "    for i in range(0, len(ecg) - window_size, window_size):\n",
    "        segment = ecg[i:i+window_size]\n",
    "        label = labels[i + window_size//2]\n",
    "\n",
    "        # Map to binary: Stress (2) â†’ 1, all others (1,3,4,5,6,7) â†’ 0\n",
    "        if label == 0:  \n",
    "            continue  # skip unlabeled\n",
    "        binary_label = 1 if label == 2 else 0\n",
    "\n",
    "        features = extract_ecg_features(segment)\n",
    "        if features:\n",
    "            X_ecg.append(features)\n",
    "            y_ecg.append(binary_label)\n",
    "\n",
    "X_ecg, y_ecg = np.array(X_ecg), np.array(y_ecg)\n",
    "print(\"âœ… ECG dataset:\", X_ecg.shape)\n",
    "print(\"Class distribution:\", Counter(y_ecg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e03967-079a-4039-8ebc-cd40e9a00b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ECG dataset: (129500, 18)\n",
      "Class distribution: Counter({np.int64(0): 102250, np.int64(1): 27250})\n"
     ]
    }
   ],
   "source": [
    "print(\"âœ… ECG dataset:\", X_ecg.shape)\n",
    "print(\"Class distribution:\", Counter(y_ecg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38e4f9d3-1292-449e-a521-7e0bca9c92bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dreaddit: (2838, 93) Counter({np.int64(1): 1488, np.int64(0): 1350})\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# ðŸ“¥ Load Dreaddit Text Dataset\n",
    "# ----------------------\n",
    "ds = load_dataset(\"andreagasparini/dreaddit\")\n",
    "df_text = ds['train'].to_pandas()\n",
    "\n",
    "y_text = df_text['label'].values\n",
    "feature_cols = [c for c in df_text.columns if c.startswith(\"lex_liwc\")]\n",
    "X_text = df_text[feature_cols].values\n",
    "\n",
    "print(\"âœ… Dreaddit:\", X_text.shape, Counter(y_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f981fe9f-0d38-4d49-aaf8-8a58f48edc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "âœ… FER-2013 embeddings: (28709, 1280)\n",
      "Label distribution: Counter({np.int64(0): 15216, np.int64(1): 13493})\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# ðŸ“¥ Load Full FER-2013 Dataset and Extract Embeddings\n",
    "# ----------------------\n",
    "fer_path = r\"C:\\Users\\chira\\OneDrive\\Desktop\\ECG BASED STRESS AND EMOTION DETECTION\\FER-2013\"\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    os.path.join(fer_path, \"train\"),\n",
    "    target_size=(48,48), color_mode=\"grayscale\",\n",
    "    class_mode=\"categorical\", batch_size=256, shuffle=True\n",
    ")\n",
    "\n",
    "# Map emotions â†’ stress\n",
    "def map_emotion_to_stress(y_emotion):\n",
    "    stress_classes = [0,1,2,4]   # Angry, Disgust, Fear, Sad\n",
    "    nonstress_classes = [3,5,6]  # Happy, Surprise, Neutral\n",
    "    return np.array([1 if np.argmax(y) in stress_classes else 0 for y in y_emotion])\n",
    "\n",
    "# CNN base for embeddings\n",
    "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, pooling=\"avg\", input_shape=(96,96,3))\n",
    "\n",
    "def preprocess_faces(images):\n",
    "    images_rgb = np.repeat(images, 3, axis=-1)         # grayscale â†’ 3 channels\n",
    "    images_resized = tf.image.resize(images_rgb, (96,96)).numpy()\n",
    "    return preprocess_input(images_resized)\n",
    "\n",
    "# Collect embeddings for ALL batches\n",
    "all_embeddings, all_labels = [], []\n",
    "for _ in range(len(train_gen)):\n",
    "    X_batch, y_batch = next(train_gen)\n",
    "    y_stress_batch = map_emotion_to_stress(y_batch)\n",
    "    X_pre = preprocess_faces(X_batch)\n",
    "    emb = base_model.predict(X_pre, verbose=0)\n",
    "    all_embeddings.append(emb)\n",
    "    all_labels.append(y_stress_batch)\n",
    "\n",
    "X_face_embeddings = np.vstack(all_embeddings)\n",
    "y_face_stress = np.hstack(all_labels)\n",
    "\n",
    "print(\"âœ… FER-2013 embeddings:\", X_face_embeddings.shape)\n",
    "print(\"Label distribution:\", Counter(y_face_stress))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c626fbf-206d-4728-b08f-ff03cc410948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:50<00:00,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded RAVDESS voice dataset\n",
      "Shape: (1440, 32)\n",
      "Label balance: [672 768]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ“¥ Load & Process RAVDESS Voice Dataset\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to your audio folder\n",
    "audio_dir = r\"C:\\Users\\chira\\OneDrive\\Desktop\\ECG BASED STRESS AND EMOTION DETECTION\\audio\"\n",
    "\n",
    "# Emotion mapping (from RAVDESS filename format)\n",
    "emotion_map = {\n",
    "    \"01\": \"neutral\", \"02\": \"calm\", \"03\": \"happy\", \"04\": \"sad\",\n",
    "    \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\", \"08\": \"surprised\"\n",
    "}\n",
    "\n",
    "def emotion_to_stress(emotion):\n",
    "    \"\"\"Map emotion â†’ stress label\"\"\"\n",
    "    stress_emotions = [\"angry\", \"fearful\", \"disgust\", \"sad\"]\n",
    "    return 1 if emotion in stress_emotions else 0\n",
    "\n",
    "def extract_features(file_path):\n",
    "    \"\"\"Extract MFCCs, Chroma, and Spectral Contrast\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=16000)\n",
    "        # Trim silence\n",
    "        y, _ = librosa.effects.trim(y)\n",
    "\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13).T, axis=0)\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)\n",
    "        contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr).T, axis=0)\n",
    "\n",
    "        return np.hstack([mfccs, chroma, contrast])\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Iterate over all audio files\n",
    "features, labels = [], []\n",
    "\n",
    "for actor_folder in tqdm(sorted(os.listdir(audio_dir))):\n",
    "    folder_path = os.path.join(audio_dir, actor_folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    for file in os.listdir(folder_path):\n",
    "        if not file.endswith(\".wav\"):\n",
    "            continue\n",
    "        emotion_code = file.split(\"-\")[2]\n",
    "        if emotion_code not in emotion_map:\n",
    "            continue\n",
    "        emotion = emotion_map[emotion_code]\n",
    "        stress_label = emotion_to_stress(emotion)\n",
    "\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        feat = extract_features(file_path)\n",
    "        if feat is not None:\n",
    "            features.append(feat)\n",
    "            labels.append(stress_label)\n",
    "\n",
    "X_voice = np.array(features)\n",
    "y_voice = np.array(labels)\n",
    "\n",
    "print(\"âœ… Loaded RAVDESS voice dataset\")\n",
    "print(\"Shape:\", X_voice.shape)\n",
    "print(\"Label balance:\", np.bincount(y_voice))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "690f15fd-2287-4c05-b925-f7f55878c2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train/Test split: (1152, 32) (288, 32)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_voice_train, X_voice_test, y_voice_train, y_voice_test = train_test_split(\n",
    "    X_voice, y_voice, test_size=0.2, random_state=42, stratify=y_voice\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_voice_train_scaled = scaler.fit_transform(X_voice_train)\n",
    "X_voice_test_scaled = scaler.transform(X_voice_test)\n",
    "\n",
    "print(\"âœ… Train/Test split:\", X_voice_train_scaled.shape, X_voice_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6b01819-f58b-4073-8f85-bdf2c29f422f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECG: Counter({np.int64(0): 81800, np.int64(1): 21800}) Counter({np.int64(0): 20450, np.int64(1): 5450})\n",
      "Text: Counter({np.int64(1): 1190, np.int64(0): 1080}) Counter({np.int64(1): 298, np.int64(0): 270})\n",
      "Face: Counter({np.int64(0): 12173, np.int64(1): 10794}) Counter({np.int64(0): 3043, np.int64(1): 2699})\n",
      "Voice: Counter({np.int64(1): 614, np.int64(0): 538}) Counter({np.int64(1): 154, np.int64(0): 134})\n",
      "âœ… All modality splits complete and saved to 'train_test_splits.pkl'\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ§© PHASE 2 â€” Train/Test Split (Updated for RAVDESS Voice Dataset)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# ------------------------------\n",
    "# ECG Dataset\n",
    "# ------------------------------\n",
    "X_ecg_train, X_ecg_test, y_ecg_train, y_ecg_test = train_test_split(\n",
    "    X_ecg, y_ecg, test_size=TEST_SIZE, stratify=y_ecg, random_state=RANDOM_STATE\n",
    ")\n",
    "print(\"ECG:\", Counter(y_ecg_train), Counter(y_ecg_test))\n",
    "\n",
    "# ------------------------------\n",
    "# Text Dataset (Dreaddit)\n",
    "# ------------------------------\n",
    "X_text_train, X_text_test, y_text_train, y_text_test = train_test_split(\n",
    "    X_text, y_text, test_size=TEST_SIZE, stratify=y_text, random_state=RANDOM_STATE\n",
    ")\n",
    "print(\"Text:\", Counter(y_text_train), Counter(y_text_test))\n",
    "\n",
    "# ------------------------------\n",
    "# Face Dataset (FER-2013)\n",
    "# ------------------------------\n",
    "X_face_train, X_face_test, y_face_train, y_face_test = train_test_split(\n",
    "    X_face_embeddings, y_face_stress, test_size=TEST_SIZE, stratify=y_face_stress, random_state=RANDOM_STATE\n",
    ")\n",
    "print(\"Face:\", Counter(y_face_train), Counter(y_face_test))\n",
    "\n",
    "# ------------------------------\n",
    "# Voice Dataset (RAVDESS)\n",
    "# ------------------------------\n",
    "X_voice_train, X_voice_test, y_voice_train, y_voice_test = train_test_split(\n",
    "    X_voice, y_voice, test_size=TEST_SIZE, stratify=y_voice, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "scaler_voice = StandardScaler()\n",
    "X_voice_train_scaled = scaler_voice.fit_transform(X_voice_train)\n",
    "X_voice_test_scaled = scaler_voice.transform(X_voice_test)\n",
    "\n",
    "print(\"Voice:\", Counter(y_voice_train), Counter(y_voice_test))\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ’¾ Optional â€” Save Train/Test Splits for Reuse\n",
    "# ============================================================\n",
    "\n",
    "save_data = {\n",
    "    \"X_ecg_train\": X_ecg_train, \"X_ecg_test\": X_ecg_test, \"y_ecg_train\": y_ecg_train, \"y_ecg_test\": y_ecg_test,\n",
    "    \"X_text_train\": X_text_train, \"X_text_test\": X_text_test, \"y_text_train\": y_text_train, \"y_text_test\": y_text_test,\n",
    "    \"X_face_train\": X_face_train, \"X_face_test\": X_face_test, \"y_face_train\": y_face_train, \"y_face_test\": y_face_test,\n",
    "    \"X_voice_train_scaled\": X_voice_train_scaled, \"X_voice_test_scaled\": X_voice_test_scaled,\n",
    "    \"y_voice_train\": y_voice_train, \"y_voice_test\": y_voice_test\n",
    "}\n",
    "\n",
    "with open(\"train_test_splits.pkl\", \"wb\") as f:\n",
    "    pickle.dump(save_data, f)\n",
    "\n",
    "print(\"âœ… All modality splits complete and saved to 'train_test_splits.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9f61f0f-c224-4576-bb90-0b9db33b1658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF ECG â†’ acc:0.9033, f1:0.7383\n",
      "HGB ECG â†’ acc:0.8847, f1:0.6828\n",
      "LogReg ECG â†’ acc:0.8096, f1:0.2779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chira\\anaconda3\\envs\\tf_env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [20:07:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB ECG â†’ acc:0.8947, f1:0.7192\n",
      "\n",
      "ECG ML summary:\n",
      "   model  accuracy        f1\n",
      "0    rf  0.903320  0.738349\n",
      "3   xgb  0.894672  0.719226\n",
      "1   hgb  0.884749  0.682751\n",
      "2   log  0.809575  0.277892\n",
      "\n",
      "Selected ECG model for fusion: rf\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# PHASE 3A-1: ECG â€” Classical ML models\n",
    "# -----------------------------\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Optional XGBoost (graceful fallback)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb_available = True\n",
    "except Exception as e:\n",
    "    print(\"xgboost not available, skipping XGBClassifier. Install via `pip install xgboost` if desired.\")\n",
    "    xgb_available = False\n",
    "\n",
    "\n",
    "\n",
    "if \"rf_ecg\" not in globals():\n",
    "    RND = 42\n",
    "    X_ecg_train = np.asarray(X_ecg_train)\n",
    "    X_ecg_test  = np.asarray(X_ecg_test)\n",
    "    y_ecg_train = np.asarray(y_ecg_train)\n",
    "    y_ecg_test  = np.asarray(y_ecg_test)\n",
    "\n",
    "    scaler_ecg = StandardScaler()\n",
    "    X_ecg_train_scaled = scaler_ecg.fit_transform(X_ecg_train)\n",
    "    X_ecg_test_scaled  = scaler_ecg.transform(X_ecg_test)\n",
    "\n",
    "    models_ecg, probs_ecg, preds_ecg, scores_ecg = {}, {}, {}, {}\n",
    "\n",
    "    # 1) Random Forest\n",
    "    rf_ecg = RandomForestClassifier(n_estimators=300, random_state=RND, n_jobs=-1)\n",
    "    rf_ecg.fit(X_ecg_train_scaled, y_ecg_train)\n",
    "    p_rf = rf_ecg.predict_proba(X_ecg_test_scaled)[:,1]\n",
    "    y_rf_pred = rf_ecg.predict(X_ecg_test_scaled)\n",
    "    models_ecg['rf'] = rf_ecg\n",
    "    probs_ecg['rf'] = p_rf\n",
    "    preds_ecg['rf'] = y_rf_pred\n",
    "    scores_ecg['rf'] = {'accuracy': accuracy_score(y_ecg_test, y_rf_pred),\n",
    "                        'f1': f1_score(y_ecg_test, y_rf_pred)}\n",
    "    print(f\"RF ECG â†’ acc:{scores_ecg['rf']['accuracy']:.4f}, f1:{scores_ecg['rf']['f1']:.4f}\")\n",
    "\n",
    "    # 2) HistGradientBoosting\n",
    "    hgb_ecg = HistGradientBoostingClassifier(random_state=RND)\n",
    "    hgb_ecg.fit(X_ecg_train_scaled, y_ecg_train)\n",
    "    p_hgb = hgb_ecg.predict_proba(X_ecg_test_scaled)[:,1]\n",
    "    y_hgb_pred = hgb_ecg.predict(X_ecg_test_scaled)\n",
    "    models_ecg['hgb'] = hgb_ecg\n",
    "    probs_ecg['hgb'] = p_hgb\n",
    "    preds_ecg['hgb'] = y_hgb_pred\n",
    "    scores_ecg['hgb'] = {'accuracy': accuracy_score(y_ecg_test, y_hgb_pred),\n",
    "                         'f1': f1_score(y_ecg_test, y_hgb_pred)}\n",
    "    print(f\"HGB ECG â†’ acc:{scores_ecg['hgb']['accuracy']:.4f}, f1:{scores_ecg['hgb']['f1']:.4f}\")\n",
    "\n",
    "    # 3) Logistic Regression\n",
    "    log_ecg = LogisticRegression(max_iter=1000, random_state=RND)\n",
    "    log_ecg.fit(X_ecg_train_scaled, y_ecg_train)\n",
    "    p_log = log_ecg.predict_proba(X_ecg_test_scaled)[:,1]\n",
    "    y_log_pred = log_ecg.predict(X_ecg_test_scaled)\n",
    "    models_ecg['log'] = log_ecg\n",
    "    probs_ecg['log'] = p_log\n",
    "    preds_ecg['log'] = y_log_pred\n",
    "    scores_ecg['log'] = {'accuracy': accuracy_score(y_ecg_test, y_log_pred),\n",
    "                         'f1': f1_score(y_ecg_test, y_log_pred)}\n",
    "    print(f\"LogReg ECG â†’ acc:{scores_ecg['log']['accuracy']:.4f}, f1:{scores_ecg['log']['f1']:.4f}\")\n",
    "\n",
    "    # 4) XGBoost (optional)\n",
    "    if xgb_available:\n",
    "        xgb_ecg = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RND)\n",
    "        xgb_ecg.fit(X_ecg_train_scaled, y_ecg_train)\n",
    "        p_xgb = xgb_ecg.predict_proba(X_ecg_test_scaled)[:,1]\n",
    "        y_xgb_pred = xgb_ecg.predict(X_ecg_test_scaled)\n",
    "        models_ecg['xgb'] = xgb_ecg\n",
    "        probs_ecg['xgb'] = p_xgb\n",
    "        preds_ecg['xgb'] = y_xgb_pred\n",
    "        scores_ecg['xgb'] = {'accuracy': accuracy_score(y_ecg_test, y_xgb_pred),\n",
    "                             'f1': f1_score(y_ecg_test, y_xgb_pred)}\n",
    "        print(f\"XGB ECG â†’ acc:{scores_ecg['xgb']['accuracy']:.4f}, f1:{scores_ecg['xgb']['f1']:.4f}\")\n",
    "\n",
    "    # Summary + best model\n",
    "    summary_df = pd.DataFrame(\n",
    "        [(k,v['accuracy'],v['f1']) for k,v in scores_ecg.items()],\n",
    "        columns=['model','accuracy','f1']\n",
    "    ).sort_values('accuracy', ascending=False)\n",
    "    print(\"\\nECG ML summary:\\n\", summary_df)\n",
    "\n",
    "    best_ml_name = max(scores_ecg.items(), key=lambda t: t[1]['f1'])[0]\n",
    "    best_ml_model_ecg = models_ecg[best_ml_name]\n",
    "    best_ml_probs_ecg = probs_ecg[best_ml_name]\n",
    "    print(f\"\\nSelected ECG model for fusion: {best_ml_name}\")\n",
    "else:\n",
    "    print(\"âš¡ Classical ECG models already loaded from pickle.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "708544ce-896b-4275-92a9-4212d1c8b2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Training Deep MLP + Hybrid Stacking for ECG...\n",
      "\u001b[1m810/810\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "âœ… ECG MLP â†’ acc: 0.9029, f1: 0.7424, auc: 0.9380\n",
      "\u001b[1m648/648\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "âœ… Hybrid (Base + MLP) â†’ acc: 0.8995, f1: 0.7343\n",
      "âœ… ECG modeling finished; objects stored in `ecg_models_for_fusion`.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHASE 3A-2: ECG â€” Deep MLP (Keras) and Hybrid Stacking\n",
    "# ============================================================\n",
    "\n",
    "# Only train if MLP not already loaded (prevents retraining after restart)\n",
    "if \"ecg_mlp\" not in globals():\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models, callbacks\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    print(\"âš™ï¸ Training Deep MLP + Hybrid Stacking for ECG...\")\n",
    "\n",
    "    # -------------------------\n",
    "    # ðŸ§  Build MLP architecture\n",
    "    # -------------------------\n",
    "    def build_ecg_mlp(input_dim):\n",
    "        model = models.Sequential([\n",
    "            layers.Input(shape=(input_dim,)),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dropout(0.25),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    ecg_input_dim = X_ecg_train_scaled.shape[1]\n",
    "    ecg_mlp = build_ecg_mlp(ecg_input_dim)\n",
    "\n",
    "    # Early stopping\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=0)\n",
    "\n",
    "    # -------------------------\n",
    "    # ðŸš€ Train MLP on ECG features\n",
    "    # -------------------------\n",
    "    history = ecg_mlp.fit(\n",
    "        X_ecg_train_scaled, y_ecg_train,\n",
    "        validation_split=0.15,\n",
    "        epochs=100,\n",
    "        batch_size=256,\n",
    "        callbacks=[es],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # ðŸ§¾ Evaluate MLP\n",
    "    # -------------------------\n",
    "    p_ecg_mlp = ecg_mlp.predict(X_ecg_test_scaled).ravel()\n",
    "    y_ecg_mlp_pred = (p_ecg_mlp > 0.5).astype(int)\n",
    "\n",
    "    mlp_acc = accuracy_score(y_ecg_test, y_ecg_mlp_pred)\n",
    "    mlp_f1  = f1_score(y_ecg_test, y_ecg_mlp_pred)\n",
    "    mlp_auc = roc_auc_score(y_ecg_test, p_ecg_mlp)\n",
    "\n",
    "    print(f\"âœ… ECG MLP â†’ acc: {mlp_acc:.4f}, f1: {mlp_f1:.4f}, auc: {mlp_auc:.4f}\")\n",
    "\n",
    "    # =====================================================\n",
    "    # ðŸ” Hybrid stacking (base model + MLP)\n",
    "    # =====================================================\n",
    "\n",
    "    # Internal split for meta training\n",
    "    X_meta_train_base, X_meta_val, y_meta_train_base, y_meta_val = train_test_split(\n",
    "        X_ecg_train_scaled, y_ecg_train, test_size=0.2,\n",
    "        stratify=y_ecg_train, random_state=RND\n",
    "    )\n",
    "\n",
    "    # Base classical model (best one from Phase 3A-1)\n",
    "    best_base = best_ml_model_ecg\n",
    "    best_base.fit(X_meta_train_base, y_meta_train_base)\n",
    "    p_base_val = best_base.predict_proba(X_meta_val)[:, 1]\n",
    "\n",
    "    # Secondary small MLP for stacking\n",
    "    ecg_mlp_small = build_ecg_mlp(ecg_input_dim)\n",
    "    ecg_mlp_small.fit(\n",
    "        X_meta_train_base, y_meta_train_base,\n",
    "        validation_split=0.1, epochs=50,\n",
    "        batch_size=128, callbacks=[es], verbose=0\n",
    "    )\n",
    "    p_mlp_val = ecg_mlp_small.predict(X_meta_val).ravel()\n",
    "\n",
    "    # Train meta logistic regression\n",
    "    meta_X = np.vstack([p_base_val, p_mlp_val]).T\n",
    "    meta_y = y_meta_val\n",
    "    meta_clf = LogisticRegression(max_iter=1000)\n",
    "    meta_clf.fit(meta_X, meta_y)\n",
    "\n",
    "    # =====================================================\n",
    "    # ðŸ§ª Evaluate Hybrid on Test Set\n",
    "    # =====================================================\n",
    "    p_base_test = best_base.predict_proba(X_ecg_test_scaled)[:, 1]\n",
    "    p_mlp_test  = p_ecg_mlp  # from full MLP model\n",
    "    meta_test_X = np.vstack([p_base_test, p_mlp_test]).T\n",
    "\n",
    "    p_meta_test = meta_clf.predict_proba(meta_test_X)[:, 1]\n",
    "    y_meta_pred = (p_meta_test > 0.5).astype(int)\n",
    "\n",
    "    hybrid_acc = accuracy_score(y_ecg_test, y_meta_pred)\n",
    "    hybrid_f1  = f1_score(y_ecg_test, y_meta_pred)\n",
    "\n",
    "    print(f\"âœ… Hybrid (Base + MLP) â†’ acc: {hybrid_acc:.4f}, f1: {hybrid_f1:.4f}\")\n",
    "\n",
    "    # =====================================================\n",
    "    # ðŸ’¾ Store Everything for Fusion & Saving\n",
    "    # =====================================================\n",
    "    ecg_models_for_fusion = {\n",
    "        \"best_ml_model\": best_ml_model_ecg,\n",
    "        \"best_ml_probs_test\": best_ml_probs_ecg,   # from Phase 3A-1\n",
    "        \"mlp_model\": ecg_mlp,\n",
    "        \"mlp_probs_test\": p_ecg_mlp,\n",
    "        \"hybrid_meta_clf\": meta_clf,\n",
    "        \"hybrid_probs_test\": p_meta_test\n",
    "    }\n",
    "\n",
    "    print(\"âœ… ECG modeling finished; objects stored in `ecg_models_for_fusion`.\")\n",
    "else:\n",
    "    print(\"âš¡ ECG MLP & Hybrid models already loaded from pickle â€” skipping retraining.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b47250bc-e06c-4eec-bb27-3db5c8660594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ ECG train/test split already in memory.\n",
      "âš¡ Using existing RandomForest ECG model.\n",
      "âœ… Calibrated RF â†’ Accuracy: 0.9027, AUC: 0.9435\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m810/810\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "âœ… Deep MLP ECG model found and predictions computed.\n",
      "âœ… Selected ECG probability source for fusion: RF (calibrated, more stable)\n",
      "ECG probability array shape: (25900,)\n",
      "âœ… ECG test probabilities prepared and stored in memory.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ©º Generate and Calibrate ECG Probabilities for Fusion\n",
    "# ============================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# âš™ï¸ Step 1: Ensure train/test split exists\n",
    "# ------------------------------------------------------------\n",
    "if not all(name in globals() for name in [\"X_ecg_train\", \"X_ecg_test\", \"y_ecg_train\", \"y_ecg_test\"]):\n",
    "    X_ecg_train, X_ecg_test, y_ecg_train, y_ecg_test = train_test_split(\n",
    "        X_ecg, y_ecg, test_size=0.2, random_state=42, stratify=y_ecg\n",
    "    )\n",
    "    print(\"âœ… Created ECG train/test split.\")\n",
    "else:\n",
    "    print(\"âš¡ ECG train/test split already in memory.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# âš™ï¸ Step 2: Calibrate Random Forest ECG probabilities\n",
    "# ------------------------------------------------------------\n",
    "if \"rf_ecg\" not in globals():\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    rf_ecg = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "    rf_ecg.fit(X_ecg_train, y_ecg_train)\n",
    "    print(\"âš™ï¸ Trained new RandomForest ECG model for calibration.\")\n",
    "else:\n",
    "    print(\"âš¡ Using existing RandomForest ECG model.\")\n",
    "\n",
    "# Calibrate model for better probabilistic output\n",
    "calibrator = CalibratedClassifierCV(rf_ecg, cv=5, method='isotonic')\n",
    "calibrator.fit(X_ecg_train, y_ecg_train)\n",
    "ecg_probs_rf = calibrator.predict_proba(X_ecg_test)[:, 1]\n",
    "\n",
    "# Evaluate RF calibration\n",
    "rf_acc = accuracy_score(y_ecg_test, calibrator.predict(X_ecg_test))\n",
    "rf_auc = roc_auc_score(y_ecg_test, ecg_probs_rf)\n",
    "print(f\"âœ… Calibrated RF â†’ Accuracy: {rf_acc:.4f}, AUC: {rf_auc:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(ecg_probs_rf, bins=20, color='steelblue', alpha=0.7)\n",
    "plt.title(\"ECG Calibrated Probability Distribution (RF)\")\n",
    "plt.xlabel(\"Predicted Stress Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# âš™ï¸ Step 3: Get Deep MLP ECG probabilities (if available)\n",
    "# ------------------------------------------------------------\n",
    "ecg_probs_mlp = None\n",
    "try:\n",
    "    if \"ecg_mlp\" in globals():\n",
    "        ecg_probs_mlp = ecg_mlp.predict(X_ecg_test).flatten()\n",
    "        print(\"âœ… Deep MLP ECG model found and predictions computed.\")\n",
    "    else:\n",
    "        print(\"âš ï¸ ECG MLP model not found; skipping MLP probabilities.\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error during MLP probability computation: {e}\")\n",
    "    ecg_probs_mlp = None\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# âš™ï¸ Step 4: Choose smoother probability distribution\n",
    "# ------------------------------------------------------------\n",
    "if ecg_probs_mlp is not None and (0.05 < np.std(ecg_probs_mlp) < np.std(ecg_probs_rf)):\n",
    "    ecg_probs_test = ecg_probs_mlp\n",
    "    selected_source = \"MLP (smoother)\"\n",
    "else:\n",
    "    ecg_probs_test = ecg_probs_rf\n",
    "    selected_source = \"RF (calibrated, more stable)\"\n",
    "\n",
    "print(f\"âœ… Selected ECG probability source for fusion: {selected_source}\")\n",
    "print(f\"ECG probability array shape: {ecg_probs_test.shape}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ðŸ’¾ Keep key variables ready for saving\n",
    "# ------------------------------------------------------------\n",
    "globals().update({\n",
    "    \"ecg_probs_test\": ecg_probs_test,\n",
    "    \"y_ecg_test\": y_ecg_test\n",
    "})\n",
    "print(\"âœ… ECG test probabilities prepared and stored in memory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08be1e94-e751-4fd0-996a-59071da836b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All ECG models and probabilities saved to ecg_model.pkl successfully.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ’¾ SAVE ALL ECG OBJECTS FOR RESTART SAFETY\n",
    "# ============================================================\n",
    "pickle.dump({\n",
    "    \"rf_ecg\": rf_ecg if \"rf_ecg\" in globals() else None,\n",
    "    \"hgb_ecg\": hgb_ecg if \"hgb_ecg\" in globals() else None,\n",
    "    \"log_ecg\": log_ecg if \"log_ecg\" in globals() else None,\n",
    "    \"xgb_ecg\": xgb_ecg if \"xgb_ecg\" in globals() else None,\n",
    "    \"ecg_mlp\": ecg_mlp if \"ecg_mlp\" in globals() else None,\n",
    "    \"meta_clf\": meta_clf if \"meta_clf\" in globals() else None,\n",
    "    \"best_ml_model_ecg\": best_ml_model_ecg,\n",
    "    \"best_ml_probs_ecg\": best_ml_probs_ecg,\n",
    "    \"ecg_probs_test\": ecg_probs_test,\n",
    "    \"y_ecg_test\": y_ecg_test\n",
    "}, open(\"ecg_model.pkl\", \"wb\"))\n",
    "\n",
    "print(\"âœ… All ECG models and probabilities saved to ecg_model.pkl successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52b0849e-4357-4975-9c96-38021abec0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Training Text (Dreaddit) classical ML models with 5-fold CV...\n",
      "LogReg | Fold 1: acc=0.767, f1=0.785\n",
      "LogReg | Fold 2: acc=0.767, f1=0.776\n",
      "LogReg | Fold 3: acc=0.767, f1=0.778\n",
      "LogReg | Fold 4: acc=0.762, f1=0.776\n",
      "LogReg | Fold 5: acc=0.758, f1=0.773\n",
      "â†’ LogReg mean acc=0.764Â±0.004, mean f1=0.778Â±0.004\n",
      "\n",
      "RandomForest | Fold 1: acc=0.733, f1=0.756\n",
      "RandomForest | Fold 2: acc=0.744, f1=0.762\n",
      "RandomForest | Fold 3: acc=0.769, f1=0.781\n",
      "RandomForest | Fold 4: acc=0.736, f1=0.753\n",
      "RandomForest | Fold 5: acc=0.767, f1=0.785\n",
      "â†’ RandomForest mean acc=0.750Â±0.015, mean f1=0.767Â±0.013\n",
      "\n",
      "XGBoost | Fold 1: acc=0.742, f1=0.762\n",
      "XGBoost | Fold 2: acc=0.731, f1=0.751\n",
      "XGBoost | Fold 3: acc=0.733, f1=0.746\n",
      "XGBoost | Fold 4: acc=0.738, f1=0.751\n",
      "XGBoost | Fold 5: acc=0.756, f1=0.773\n",
      "â†’ XGBoost mean acc=0.740Â±0.009, mean f1=0.757Â±0.010\n",
      "\n",
      "ðŸ† Best classical text model by mean CV-F1: log\n",
      "\n",
      "ðŸ“Š Test set report for best classical text model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.74      0.74       270\n",
      "           1       0.76      0.77      0.77       298\n",
      "\n",
      "    accuracy                           0.76       568\n",
      "   macro avg       0.75      0.75      0.75       568\n",
      "weighted avg       0.76      0.76      0.76       568\n",
      "\n",
      "Confusion Matrix:\n",
      " [[199  71]\n",
      " [ 68 230]]\n",
      "âœ… Text classical model ready for fusion.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHASE 3B-1 â€” Text (Dreaddit): Classical ML with 5-fold CV\n",
    "# ============================================================\n",
    "\n",
    "# Train only if not already loaded from pickle\n",
    "if \"text_ml_model_best\" not in globals():\n",
    "    import numpy as np, pandas as pd\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "    from xgboost import XGBClassifier\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    print(\"âš™ï¸ Training Text (Dreaddit) classical ML models with 5-fold CV...\")\n",
    "\n",
    "    RND = 42\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RND)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # âš™ï¸ Cross-validation helper\n",
    "    # --------------------------------------------------------\n",
    "    def crossval_model(clf, X, y, name):\n",
    "        accs, f1s = [], []\n",
    "        for fold, (tr, va) in enumerate(kf.split(X, y), 1):\n",
    "            sc = StandardScaler()\n",
    "            X_tr, X_va = sc.fit_transform(X[tr]), sc.transform(X[va])\n",
    "            clf.fit(X_tr, y[tr])\n",
    "            y_pred = clf.predict(X_va)\n",
    "            accs.append(accuracy_score(y[va], y_pred))\n",
    "            f1s.append(f1_score(y[va], y_pred))\n",
    "            print(f\"{name} | Fold {fold}: acc={accs[-1]:.3f}, f1={f1s[-1]:.3f}\")\n",
    "        print(f\"â†’ {name} mean acc={np.mean(accs):.3f}Â±{np.std(accs):.3f}, \"\n",
    "              f\"mean f1={np.mean(f1s):.3f}Â±{np.std(f1s):.3f}\\n\")\n",
    "        return np.mean(accs), np.mean(f1s)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # âš™ï¸ Prepare data\n",
    "    # --------------------------------------------------------\n",
    "    X = np.asarray(X_text_train)\n",
    "    y = np.asarray(y_text_train)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # âš™ï¸ 1) Logistic Regression\n",
    "    # --------------------------------------------------------\n",
    "    log_model = LogisticRegression(max_iter=2000, random_state=RND)\n",
    "    cv_acc_log, cv_f1_log = crossval_model(log_model, X, y, \"LogReg\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # âš™ï¸ 2) Random Forest\n",
    "    # --------------------------------------------------------\n",
    "    rf_model = RandomForestClassifier(n_estimators=300, random_state=RND, n_jobs=-1)\n",
    "    cv_acc_rf, cv_f1_rf = crossval_model(rf_model, X, y, \"RandomForest\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # âš™ï¸ 3) XGBoost (optional)\n",
    "    # --------------------------------------------------------\n",
    "    try:\n",
    "        xgb_model = XGBClassifier(\n",
    "            use_label_encoder=False, eval_metric=\"logloss\",\n",
    "            random_state=RND, verbosity=0\n",
    "        )\n",
    "        cv_acc_xgb, cv_f1_xgb = crossval_model(xgb_model, X, y, \"XGBoost\")\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ XGBoost not available, skipping:\", e)\n",
    "        xgb_model, cv_acc_xgb, cv_f1_xgb = None, np.nan, np.nan\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # âš™ï¸ Choose best by CV F1\n",
    "    # --------------------------------------------------------\n",
    "    scores = {\"log\": cv_f1_log, \"rf\": cv_f1_rf, \"xgb\": cv_f1_xgb}\n",
    "    best_name = max(scores.items(), key=lambda kv: (0 if np.isnan(kv[1]) else kv[1]))[0]\n",
    "    print(f\"ðŸ† Best classical text model by mean CV-F1: {best_name}\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # âš™ï¸ Retrain best on full training set\n",
    "    # --------------------------------------------------------\n",
    "    sc_text = StandardScaler()\n",
    "    X_tr_scaled = sc_text.fit_transform(X_text_train)\n",
    "    X_te_scaled = sc_text.transform(X_text_test)\n",
    "\n",
    "    if best_name == \"rf\":   final_clf = rf_model\n",
    "    elif best_name == \"xgb\" and xgb_model is not None: final_clf = xgb_model\n",
    "    else:                   final_clf = log_model\n",
    "\n",
    "    final_clf.fit(X_tr_scaled, y_text_train)\n",
    "    y_pred_test = final_clf.predict(X_te_scaled)\n",
    "    y_prob_test = (final_clf.predict_proba(X_te_scaled)[:, 1]\n",
    "                   if hasattr(final_clf, \"predict_proba\") else y_pred_test)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # ðŸ§¾ Evaluate on test set\n",
    "    # --------------------------------------------------------\n",
    "    print(\"\\nðŸ“Š Test set report for best classical text model:\")\n",
    "    print(classification_report(y_text_test, y_pred_test))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_text_test, y_pred_test))\n",
    "\n",
    "    text_ml_probs_test = y_prob_test\n",
    "    text_ml_model_best = final_clf\n",
    "    print(\"âœ… Text classical model ready for fusion.\")\n",
    "\n",
    "else:\n",
    "    print(\"âš¡ Text classical model already loaded from pickle â€” skipping retraining.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a9f5689-82dd-4e48-97b9-e64b433b9ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Training Deep MLP for Text (Dreaddit) with 5-fold CV...\n",
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Fold 1: acc=0.758, f1=0.776\n",
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Fold 2: acc=0.764, f1=0.774\n",
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Fold 3: acc=0.773, f1=0.778\n",
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Fold 4: acc=0.747, f1=0.752\n",
      "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Fold 5: acc=0.764, f1=0.780\n",
      "\n",
      "âœ… Text MLP CV â†’ mean acc=0.761Â±0.009, mean f1=0.772Â±0.010\n",
      "\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "ðŸ“Š Test set MLP performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.73       270\n",
      "           1       0.75      0.77      0.76       298\n",
      "\n",
      "    accuracy                           0.74       568\n",
      "   macro avg       0.74      0.74      0.74       568\n",
      "weighted avg       0.74      0.74      0.74       568\n",
      "\n",
      "âœ… Text modeling finished; objects saved to `text_models_for_fusion`.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHASE 3B-2 â€” Text (Dreaddit): Deep MLP with 5-fold CV\n",
    "# ============================================================\n",
    "\n",
    "# Only train if MLP not already loaded (prevents retraining after restart)\n",
    "if \"text_mlp_model\" not in globals():\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models, callbacks\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "    RND = 42\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RND)\n",
    "\n",
    "    print(\"âš™ï¸ Training Deep MLP for Text (Dreaddit) with 5-fold CV...\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # ðŸ§  Define model architecture\n",
    "    # ------------------------------------------------------------\n",
    "    def build_text_mlp(input_dim):\n",
    "        model = models.Sequential([\n",
    "            layers.Input(shape=(input_dim,)),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dropout(0.4),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # âš™ï¸ Cross-validation training\n",
    "    # ------------------------------------------------------------\n",
    "    X = np.asarray(X_text_train)\n",
    "    y = np.asarray(y_text_train)\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    accs, f1s = [], []\n",
    "    for fold, (tr, va) in enumerate(kf.split(X, y), 1):\n",
    "        sc = StandardScaler()\n",
    "        X_tr, X_va = sc.fit_transform(X[tr]), sc.transform(X[va])\n",
    "        model = build_text_mlp(input_dim)\n",
    "        es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=0)\n",
    "        model.fit(X_tr, y[tr], validation_data=(X_va, y[va]),\n",
    "                  epochs=30, batch_size=64, callbacks=[es], verbose=0)\n",
    "        y_pred = (model.predict(X_va).ravel() > 0.5).astype(int)\n",
    "        acc = accuracy_score(y[va], y_pred)\n",
    "        f1 = f1_score(y[va], y_pred)\n",
    "        accs.append(acc)\n",
    "        f1s.append(f1)\n",
    "        print(f\"Fold {fold}: acc={acc:.3f}, f1={f1:.3f}\")\n",
    "\n",
    "    print(f\"\\nâœ… Text MLP CV â†’ mean acc={np.mean(accs):.3f}Â±{np.std(accs):.3f}, \"\n",
    "          f\"mean f1={np.mean(f1s):.3f}Â±{np.std(f1s):.3f}\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # ðŸš€ Retrain on full training set â†’ test\n",
    "    # ------------------------------------------------------------\n",
    "    sc_text_mlp = StandardScaler()\n",
    "    X_tr_scaled = sc_text_mlp.fit_transform(X_text_train)\n",
    "    X_te_scaled = sc_text_mlp.transform(X_text_test)\n",
    "\n",
    "    text_mlp_model = build_text_mlp(input_dim)\n",
    "    es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True, verbose=0)\n",
    "    text_mlp_model.fit(\n",
    "        X_tr_scaled, y_text_train,\n",
    "        validation_split=0.15,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        callbacks=[es],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # ðŸ§¾ Evaluate MLP on test set\n",
    "    # ------------------------------------------------------------\n",
    "    y_prob_test_mlp = text_mlp_model.predict(X_te_scaled).ravel()\n",
    "    y_pred_test_mlp = (y_prob_test_mlp > 0.5).astype(int)\n",
    "\n",
    "    print(\"\\nðŸ“Š Test set MLP performance:\")\n",
    "    print(classification_report(y_text_test, y_pred_test_mlp))\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # ðŸ’¾ Store for fusion and saving\n",
    "    # ------------------------------------------------------------\n",
    "    text_mlp_probs_test = y_prob_test_mlp\n",
    "\n",
    "    text_models_for_fusion = {\n",
    "        \"best_ml_model\": text_ml_model_best,    # from PHASE 3B-1\n",
    "        \"ml_probs_test\": text_ml_probs_test,\n",
    "        \"mlp_model\": text_mlp_model,\n",
    "        \"mlp_probs_test\": text_mlp_probs_test\n",
    "    }\n",
    "\n",
    "    print(\"âœ… Text modeling finished; objects saved to `text_models_for_fusion`.\")\n",
    "else:\n",
    "    print(\"âš¡ Text MLP model already loaded from pickle â€” skipping retraining.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10ffcd7e-b174-4b25-8dc1-cf0dfe1472b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All Text models and probabilities saved successfully to 'text_model.pkl'.\n",
      "Saved keys: ['text_ml_model_best', 'text_ml_probs_test', 'text_mlp_model', 'text_mlp_probs_test', 'y_text_test']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ’¾ SAVE ALL TEXT MODELS AND PROBABILITIES FOR RESTART SAFETY\n",
    "# ============================================================\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Ensure all variables exist before dumping\n",
    "to_save = {\n",
    "    \"text_ml_model_best\": text_ml_model_best if \"text_ml_model_best\" in globals() else None,\n",
    "    \"text_ml_probs_test\": text_ml_probs_test if \"text_ml_probs_test\" in globals() else None,\n",
    "    \"text_mlp_model\": text_mlp_model if \"text_mlp_model\" in globals() else None,\n",
    "    \"text_mlp_probs_test\": text_mlp_probs_test if \"text_mlp_probs_test\" in globals() else None,\n",
    "    \"y_text_test\": y_text_test if \"y_text_test\" in globals() else None\n",
    "}\n",
    "\n",
    "pickle.dump(to_save, open(\"text_model.pkl\", \"wb\"))\n",
    "\n",
    "print(\"âœ… All Text models and probabilities saved successfully to 'text_model.pkl'.\")\n",
    "print(\"Saved keys:\", list(to_save.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66e564fa-fe40-4e8b-be4d-7eb400f3ac7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Training Face (FER embeddings) classical ML models with 5-fold CV...\n",
      "RandomForest | Fold 1: acc=0.614, f1=0.532\n",
      "RandomForest | Fold 2: acc=0.605, f1=0.519\n",
      "RandomForest | Fold 3: acc=0.614, f1=0.527\n",
      "RandomForest | Fold 4: acc=0.612, f1=0.523\n",
      "RandomForest | Fold 5: acc=0.626, f1=0.540\n",
      "â†’ RandomForest mean acc=0.614Â±0.007, mean f1=0.528Â±0.007\n",
      "\n",
      "XGBoost | Fold 1: acc=0.604, f1=0.564\n",
      "XGBoost | Fold 2: acc=0.601, f1=0.560\n",
      "XGBoost | Fold 3: acc=0.598, f1=0.553\n",
      "XGBoost | Fold 4: acc=0.590, f1=0.550\n",
      "XGBoost | Fold 5: acc=0.605, f1=0.563\n",
      "â†’ XGBoost mean acc=0.600Â±0.005, mean f1=0.558Â±0.006\n",
      "\n",
      "ðŸ† Best classical face model by CV F1: XGB\n",
      "\n",
      "ðŸ“Š Test set performance for best face classical model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.66      0.63      3043\n",
      "           1       0.57      0.52      0.55      2699\n",
      "\n",
      "    accuracy                           0.59      5742\n",
      "   macro avg       0.59      0.59      0.59      5742\n",
      "weighted avg       0.59      0.59      0.59      5742\n",
      "\n",
      "Confusion Matrix:\n",
      " [[2004 1039]\n",
      " [1297 1402]]\n",
      "âœ… Face classical model ready for fusion.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHASE 3C-1 â€” Face (FER Embeddings): Classical ML with 5-fold CV\n",
    "# ============================================================\n",
    "\n",
    "# Train only if not already loaded (prevents retraining after restart)\n",
    "if \"face_ml_model_best\" not in globals():\n",
    "    import numpy as np, pandas as pd\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "    # Optional XGBoost\n",
    "    try:\n",
    "        from xgboost import XGBClassifier\n",
    "        xgb_available = True\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ XGBoost not available, skipping XGBClassifier:\", e)\n",
    "        xgb_available = False\n",
    "\n",
    "    RND = 42\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RND)\n",
    "\n",
    "    print(\"âš™ï¸ Training Face (FER embeddings) classical ML models with 5-fold CV...\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # âš™ï¸ Cross-validation helper\n",
    "    # ------------------------------------------------------------\n",
    "    def crossval_face_model(clf, X, y, name):\n",
    "        accs, f1s = [], []\n",
    "        for fold, (tr, va) in enumerate(kf.split(X, y), 1):\n",
    "            sc = StandardScaler()\n",
    "            X_tr, X_va = sc.fit_transform(X[tr]), sc.transform(X[va])\n",
    "            clf.fit(X_tr, y[tr])\n",
    "            y_pred = clf.predict(X_va)\n",
    "            accs.append(accuracy_score(y[va], y_pred))\n",
    "            f1s.append(f1_score(y[va], y_pred))\n",
    "            print(f\"{name} | Fold {fold}: acc={accs[-1]:.3f}, f1={f1s[-1]:.3f}\")\n",
    "        print(f\"â†’ {name} mean acc={np.mean(accs):.3f}Â±{np.std(accs):.3f}, \"\n",
    "              f\"mean f1={np.mean(f1s):.3f}Â±{np.std(f1s):.3f}\\n\")\n",
    "        return np.mean(accs), np.mean(f1s)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # âš™ï¸ Prepare data\n",
    "    # ------------------------------------------------------------\n",
    "    X = np.asarray(X_face_train)\n",
    "    y = np.asarray(y_face_train)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # âš™ï¸ 1) Random Forest\n",
    "    # ------------------------------------------------------------\n",
    "    rf_face = RandomForestClassifier(n_estimators=300, random_state=RND, n_jobs=-1)\n",
    "    cv_acc_rf, cv_f1_rf = crossval_face_model(rf_face, X, y, \"RandomForest\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # âš™ï¸ 2) XGBoost (if available)\n",
    "    # ------------------------------------------------------------\n",
    "    if xgb_available:\n",
    "        xgb_face = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\",\n",
    "                                 random_state=RND, verbosity=0)\n",
    "        cv_acc_xgb, cv_f1_xgb = crossval_face_model(xgb_face, X, y, \"XGBoost\")\n",
    "    else:\n",
    "        cv_acc_xgb, cv_f1_xgb = np.nan, np.nan\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # ðŸ† Choose best model by mean CV F1\n",
    "    # ------------------------------------------------------------\n",
    "    best_name = \"rf\" if (cv_f1_rf >= (cv_f1_xgb if not np.isnan(cv_f1_xgb) else -1)) else \"xgb\"\n",
    "    print(f\"ðŸ† Best classical face model by CV F1: {best_name.upper()}\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # ðŸš€ Train best model on full data â†’ test evaluation\n",
    "    # ------------------------------------------------------------\n",
    "    sc_face = StandardScaler()\n",
    "    X_face_train_scaled = sc_face.fit_transform(X_face_train)\n",
    "    X_face_test_scaled  = sc_face.transform(X_face_test)\n",
    "\n",
    "    best_clf = rf_face if best_name == \"rf\" else xgb_face\n",
    "    best_clf.fit(X_face_train_scaled, y_face_train)\n",
    "    y_face_pred = best_clf.predict(X_face_test_scaled)\n",
    "    y_face_prob = best_clf.predict_proba(X_face_test_scaled)[:, 1]\n",
    "\n",
    "    print(\"\\nðŸ“Š Test set performance for best face classical model:\")\n",
    "    print(classification_report(y_face_test, y_face_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_face_test, y_face_pred))\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # ðŸ’¾ Store for fusion / saving\n",
    "    # ------------------------------------------------------------\n",
    "    face_ml_model_best = best_clf\n",
    "    face_ml_probs_test = y_face_prob\n",
    "\n",
    "    print(\"âœ… Face classical model ready for fusion.\")\n",
    "\n",
    "else:\n",
    "    print(\"âš¡ Face classical model already loaded from pickle â€” skipping retraining.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ab5ea96-3c60-4b97-ad2d-ca0900fd5a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Training Deep MLP for Face (FER embeddings) with 5-fold CV...\n",
      "\u001b[1m144/144\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 1: acc=0.604, f1=0.564\n",
      "\u001b[1m144/144\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 2: acc=0.608, f1=0.546\n",
      "\u001b[1m144/144\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 3: acc=0.607, f1=0.556\n",
      "\u001b[1m144/144\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Fold 4: acc=0.600, f1=0.491\n",
      "\u001b[1m144/144\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Fold 5: acc=0.599, f1=0.514\n",
      "\n",
      "âœ… Face MLP CV â†’ mean acc=0.604Â±0.004, mean f1=0.534Â±0.028\n",
      "\u001b[1m180/180\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "ðŸ“Š Test set Face MLP performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.65      0.63      3043\n",
      "           1       0.58      0.55      0.56      2699\n",
      "\n",
      "    accuracy                           0.60      5742\n",
      "   macro avg       0.60      0.60      0.60      5742\n",
      "weighted avg       0.60      0.60      0.60      5742\n",
      "\n",
      "âœ… Face modeling finished; objects saved to `face_models_for_fusion`.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHASE 3C-2 â€” Face (FER Embeddings): Deep MLP with 5-fold CV\n",
    "# ============================================================\n",
    "\n",
    "# Only train if MLP not already loaded (prevents retraining after restart)\n",
    "if \"face_mlp_model\" not in globals():\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models, callbacks\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "    RND = 42\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RND)\n",
    "\n",
    "    print(\"âš™ï¸ Training Deep MLP for Face (FER embeddings) with 5-fold CV...\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # ðŸ§  Define Face MLP architecture\n",
    "    # ------------------------------------------------------------\n",
    "    def build_face_mlp(input_dim):\n",
    "        model = models.Sequential([\n",
    "            layers.Input(shape=(input_dim,)),\n",
    "            layers.Dense(512, activation='relu'),\n",
    "            layers.Dropout(0.4),\n",
    "            layers.Dense(256, activation='relu'),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # âš™ï¸ Cross-validation training\n",
    "    # ------------------------------------------------------------\n",
    "    X = np.asarray(X_face_train)\n",
    "    y = np.asarray(y_face_train)\n",
    "    input_dim = X.shape[1]\n",
    "\n",
    "    accs, f1s = [], []\n",
    "    for fold, (tr, va) in enumerate(kf.split(X, y), 1):\n",
    "        sc = StandardScaler()\n",
    "        X_tr, X_va = sc.fit_transform(X[tr]), sc.transform(X[va])\n",
    "        model = build_face_mlp(input_dim)\n",
    "        es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=0)\n",
    "        model.fit(X_tr, y[tr], validation_data=(X_va, y[va]),\n",
    "                  epochs=30, batch_size=64, callbacks=[es], verbose=0)\n",
    "        y_pred = (model.predict(X_va).ravel() > 0.5).astype(int)\n",
    "        acc, f1 = accuracy_score(y[va], y_pred), f1_score(y[va], y_pred)\n",
    "        accs.append(acc); f1s.append(f1)\n",
    "        print(f\"Fold {fold}: acc={acc:.3f}, f1={f1:.3f}\")\n",
    "\n",
    "    print(f\"\\nâœ… Face MLP CV â†’ mean acc={np.mean(accs):.3f}Â±{np.std(accs):.3f}, \"\n",
    "          f\"mean f1={np.mean(f1s):.3f}Â±{np.std(f1s):.3f}\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # ðŸš€ Retrain on full training set â†’ test\n",
    "    # ------------------------------------------------------------\n",
    "    sc_face_mlp = StandardScaler()\n",
    "    X_face_train_scaled = sc_face_mlp.fit_transform(X_face_train)\n",
    "    X_face_test_scaled  = sc_face_mlp.transform(X_face_test)\n",
    "\n",
    "    face_mlp_model = build_face_mlp(input_dim)\n",
    "    es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True, verbose=0)\n",
    "    face_mlp_model.fit(\n",
    "        X_face_train_scaled, y_face_train,\n",
    "        validation_split=0.15,\n",
    "        epochs=50, batch_size=64,\n",
    "        callbacks=[es],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # ðŸ§¾ Evaluate MLP on test set\n",
    "    # ------------------------------------------------------------\n",
    "    y_face_prob_mlp = face_mlp_model.predict(X_face_test_scaled).ravel()\n",
    "    y_face_pred_mlp = (y_face_prob_mlp > 0.5).astype(int)\n",
    "\n",
    "    print(\"\\nðŸ“Š Test set Face MLP performance:\")\n",
    "    print(classification_report(y_face_test, y_face_pred_mlp))\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # ðŸ’¾ Store for fusion and saving\n",
    "    # ------------------------------------------------------------\n",
    "    face_mlp_probs_test = y_face_prob_mlp\n",
    "\n",
    "    face_models_for_fusion = {\n",
    "        \"best_ml_model\": face_ml_model_best,   # from Phase 3C-1\n",
    "        \"ml_probs_test\": face_ml_probs_test,\n",
    "        \"mlp_model\": face_mlp_model,\n",
    "        \"mlp_probs_test\": face_mlp_probs_test\n",
    "    }\n",
    "\n",
    "    print(\"âœ… Face modeling finished; objects saved to `face_models_for_fusion`.\")\n",
    "else:\n",
    "    print(\"âš¡ Face MLP model already loaded from pickle â€” skipping retraining.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a016e81-c225-4447-9146-d884b4cada1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All Face models and probabilities saved successfully to 'face_model.pkl'.\n",
      "Saved keys: ['face_ml_model_best', 'face_ml_probs_test', 'face_mlp_model', 'face_mlp_probs_test', 'y_face_test']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ’¾ SAVE ALL FACE MODELS AND PROBABILITIES FOR RESTART SAFETY\n",
    "# ============================================================\n",
    "\n",
    "import pickle\n",
    "\n",
    "to_save = {\n",
    "    \"face_ml_model_best\": face_ml_model_best if \"face_ml_model_best\" in globals() else None,\n",
    "    \"face_ml_probs_test\": face_ml_probs_test if \"face_ml_probs_test\" in globals() else None,\n",
    "    \"face_mlp_model\": face_mlp_model if \"face_mlp_model\" in globals() else None,\n",
    "    \"face_mlp_probs_test\": face_mlp_probs_test if \"face_mlp_probs_test\" in globals() else None,\n",
    "    \"y_face_test\": y_face_test if \"y_face_test\" in globals() else None\n",
    "}\n",
    "\n",
    "pickle.dump(to_save, open(\"face_model.pkl\", \"wb\"))\n",
    "\n",
    "print(\"âœ… All Face models and probabilities saved successfully to 'face_model.pkl'.\")\n",
    "print(\"Saved keys:\", list(to_save.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "096d024c-8ecf-4473-9443-432908a02489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Training Voice (RAVDESS) Classical ML Models...\n",
      "ðŸŽ¯ RandomForest â†’ acc: 0.6944, f1: 0.7047\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.71      0.68       134\n",
      "           1       0.73      0.68      0.70       154\n",
      "\n",
      "    accuracy                           0.69       288\n",
      "   macro avg       0.69      0.70      0.69       288\n",
      "weighted avg       0.70      0.69      0.69       288\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 95  39]\n",
      " [ 49 105]]\n",
      "\n",
      "ðŸŽ¯ XGBoost â†’ acc: 0.7361, f1: 0.7467\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.75      0.72       134\n",
      "           1       0.77      0.73      0.75       154\n",
      "\n",
      "    accuracy                           0.74       288\n",
      "   macro avg       0.74      0.74      0.74       288\n",
      "weighted avg       0.74      0.74      0.74       288\n",
      "\n",
      "Confusion Matrix:\n",
      " [[100  34]\n",
      " [ 42 112]]\n",
      "\n",
      "ðŸ† Selected Voice Classical Model: XGBoost\n",
      "âœ… Voice classical model ready for fusion.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHASE 3D-1 â€” Voice (RAVDESS): Classical ML\n",
    "# ============================================================\n",
    "\n",
    "# Train only if not already loaded (prevents retraining after restart)\n",
    "if \"voice_ml_model_best\" not in globals():\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "    print(\"âš™ï¸ Training Voice (RAVDESS) Classical ML Models...\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # ðŸ§© Prepare scaled inputs\n",
    "    # ------------------------------------------------------------\n",
    "    X_train = np.asarray(X_voice_train_scaled)\n",
    "    X_test  = np.asarray(X_voice_test_scaled)\n",
    "    y_train = np.asarray(y_voice_train)\n",
    "    y_test  = np.asarray(y_voice_test)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # âš™ï¸ 1ï¸âƒ£ Random Forest\n",
    "    # ------------------------------------------------------------\n",
    "    rf_voice = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "    rf_voice.fit(X_train, y_train)\n",
    "    y_pred_rf = rf_voice.predict(X_test)\n",
    "    y_prob_rf = rf_voice.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    rf_acc = accuracy_score(y_test, y_pred_rf)\n",
    "    rf_f1  = f1_score(y_test, y_pred_rf)\n",
    "    print(f\"ðŸŽ¯ RandomForest â†’ acc: {rf_acc:.4f}, f1: {rf_f1:.4f}\")\n",
    "    print(classification_report(y_test, y_pred_rf))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # âš™ï¸ 2ï¸âƒ£ XGBoost (optional)\n",
    "    # ------------------------------------------------------------\n",
    "    try:\n",
    "        from xgboost import XGBClassifier\n",
    "        xgb_voice = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "        xgb_voice.fit(X_train, y_train)\n",
    "        y_pred_xgb = xgb_voice.predict(X_test)\n",
    "        y_prob_xgb = xgb_voice.predict_proba(X_test)[:, 1]\n",
    "        xgb_acc = accuracy_score(y_test, y_pred_xgb)\n",
    "        xgb_f1  = f1_score(y_test, y_pred_xgb)\n",
    "        print(f\"\\nðŸŽ¯ XGBoost â†’ acc: {xgb_acc:.4f}, f1: {xgb_f1:.4f}\")\n",
    "        print(classification_report(y_test, y_pred_xgb))\n",
    "        print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))\n",
    "        xgb_available = True\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ XGBoost not available, skipping:\", e)\n",
    "        xgb_voice, y_prob_xgb, xgb_f1 = None, None, -1\n",
    "        xgb_available = False\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # ðŸ† Select best model by F1\n",
    "    # ------------------------------------------------------------\n",
    "    if xgb_available and xgb_f1 > rf_f1:\n",
    "        voice_ml_model_best = xgb_voice\n",
    "        voice_ml_probs_test = y_prob_xgb\n",
    "        best_model_name = \"XGBoost\"\n",
    "    else:\n",
    "        voice_ml_model_best = rf_voice\n",
    "        voice_ml_probs_test = y_prob_rf\n",
    "        best_model_name = \"RandomForest\"\n",
    "\n",
    "    print(f\"\\nðŸ† Selected Voice Classical Model: {best_model_name}\")\n",
    "    print(\"âœ… Voice classical model ready for fusion.\")\n",
    "\n",
    "else:\n",
    "    print(\"âš¡ Voice classical model already loaded from pickle â€” skipping retraining.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c61b2cac-df2a-43f8-91e6-4d067dec97c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Training Voice Deep MLP (RAVDESS)...\n",
      "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Voice Deep MLP Performance:\n",
      "Accuracy: 0.681 | AUC: 0.765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.659     0.649     0.654       134\n",
      "           1      0.699     0.708     0.703       154\n",
      "\n",
      "    accuracy                          0.681       288\n",
      "   macro avg      0.679     0.679     0.679       288\n",
      "weighted avg      0.680     0.681     0.680       288\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 87  47]\n",
      " [ 45 109]]\n",
      "âœ… Voice modeling finished; objects saved to `voice_models_for_fusion`.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHASE 3D-2 â€” Voice (RAVDESS): Deep Learning Model (MLP)\n",
    "# ============================================================\n",
    "\n",
    "# Only train if MLP not already loaded (prevents retraining after restart)\n",
    "if \"voice_mlp_model\" not in globals():\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    from sklearn.metrics import (\n",
    "        classification_report, confusion_matrix,\n",
    "        accuracy_score, roc_auc_score\n",
    "    )\n",
    "\n",
    "    print(\"âš™ï¸ Training Voice Deep MLP (RAVDESS)...\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # ðŸŽ›ï¸ Build MLP architecture\n",
    "    # ------------------------------------------------------------\n",
    "    def build_voice_mlp(input_dim):\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "\n",
    "            Dense(64, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "\n",
    "            Dense(32, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    input_dim = X_voice_train_scaled.shape[1]\n",
    "    voice_mlp_model = build_voice_mlp(input_dim)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # ðŸ§  Train MLP\n",
    "    # ------------------------------------------------------------\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)\n",
    "\n",
    "    history = voice_mlp_model.fit(\n",
    "        X_voice_train_scaled, y_voice_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # ðŸ“Š Evaluate MLP\n",
    "    # ------------------------------------------------------------\n",
    "    y_pred_mlp_prob = voice_mlp_model.predict(X_voice_test_scaled).ravel()\n",
    "    y_pred_mlp = (y_pred_mlp_prob > 0.5).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_voice_test, y_pred_mlp)\n",
    "    auc = roc_auc_score(y_voice_test, y_pred_mlp_prob)\n",
    "\n",
    "    print(\"\\nâœ… Voice Deep MLP Performance:\")\n",
    "    print(f\"Accuracy: {acc:.3f} | AUC: {auc:.3f}\")\n",
    "    print(classification_report(y_voice_test, y_pred_mlp, digits=3))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_voice_test, y_pred_mlp))\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # ðŸ’¾ Save Keras model (optional .h5)\n",
    "    # ------------------------------------------------------------\n",
    "    voice_mlp_model.save(\"voice_mlp_model_ravdess.h5\")\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # ðŸ“¦ Store for fusion + saving\n",
    "    # ------------------------------------------------------------\n",
    "    voice_mlp_probs_test = y_pred_mlp_prob\n",
    "\n",
    "    voice_models_for_fusion = {\n",
    "        \"best_ml_model\": voice_ml_model_best,   # from Phase 3D-1\n",
    "        \"ml_probs_test\": voice_ml_probs_test,\n",
    "        \"mlp_model\": voice_mlp_model,\n",
    "        \"mlp_probs_test\": voice_mlp_probs_test\n",
    "    }\n",
    "\n",
    "    print(\"âœ… Voice modeling finished; objects saved to `voice_models_for_fusion`.\")\n",
    "else:\n",
    "    print(\"âš¡ Voice MLP model already loaded from pickle â€” skipping retraining.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe6c5bd6-7de4-4636-b2f3-566fe82635cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All Voice models and probabilities saved successfully to 'voice_model.pkl'.\n",
      "Saved keys: ['voice_ml_model_best', 'voice_ml_probs_test', 'voice_mlp_model', 'voice_mlp_probs_test', 'y_voice_test']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ’¾ SAVE ALL VOICE MODELS AND PROBABILITIES FOR RESTART SAFETY\n",
    "# ============================================================\n",
    "\n",
    "import pickle\n",
    "\n",
    "to_save = {\n",
    "    \"voice_ml_model_best\": voice_ml_model_best if \"voice_ml_model_best\" in globals() else None,\n",
    "    \"voice_ml_probs_test\": voice_ml_probs_test if \"voice_ml_probs_test\" in globals() else None,\n",
    "    \"voice_mlp_model\": voice_mlp_model if \"voice_mlp_model\" in globals() else None,\n",
    "    \"voice_mlp_probs_test\": voice_mlp_probs_test if \"voice_mlp_probs_test\" in globals() else None,\n",
    "    \"y_voice_test\": y_voice_test if \"y_voice_test\" in globals() else None\n",
    "}\n",
    "\n",
    "pickle.dump(to_save, open(\"voice_model.pkl\", \"wb\"))\n",
    "\n",
    "print(\"âœ… All Voice models and probabilities saved successfully to 'voice_model.pkl'.\")\n",
    "print(\"Saved keys:\", list(to_save.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "298e3608-51cd-4087-8a15-0cd34f6def6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Fusion input probabilities saved to 'fusion_inputs.pkl'.\n",
      "Saved keys: ['ecg_probs_test', 'text_probs_test', 'face_probs_test', 'voice_mlp_probs_test', 'y_voice_test']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ’¾ SAVE FUSION INPUT PROBABILITIES FOR FINAL FUSION PHASE\n",
    "# ============================================================\n",
    "\n",
    "import pickle\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ðŸ§© Collect only available modality probabilities safely\n",
    "# ------------------------------------------------------------\n",
    "fusion_inputs = {}\n",
    "\n",
    "# ECG\n",
    "if \"ecg_probs_test\" in globals():\n",
    "    fusion_inputs[\"ecg_probs_test\"] = ecg_probs_test\n",
    "else:\n",
    "    print(\"âš ï¸ ECG probabilities not found â€” skipping.\")\n",
    "\n",
    "# TEXT\n",
    "if \"text_mlp_probs_test\" in globals():\n",
    "    fusion_inputs[\"text_probs_test\"] = text_mlp_probs_test\n",
    "elif \"text_ml_probs_test\" in globals():\n",
    "    fusion_inputs[\"text_probs_test\"] = text_ml_probs_test\n",
    "else:\n",
    "    print(\"âš ï¸ Text probabilities not found â€” skipping.\")\n",
    "\n",
    "# FACE\n",
    "if \"face_mlp_probs_test\" in globals():\n",
    "    fusion_inputs[\"face_probs_test\"] = face_mlp_probs_test\n",
    "elif \"face_ml_probs_test\" in globals():\n",
    "    fusion_inputs[\"face_probs_test\"] = face_ml_probs_test\n",
    "else:\n",
    "    print(\"âš ï¸ Face probabilities not found â€” skipping.\")\n",
    "\n",
    "# VOICE\n",
    "if \"voice_mlp_probs_test\" in globals():\n",
    "    fusion_inputs[\"voice_mlp_probs_test\"] = voice_mlp_probs_test\n",
    "elif \"voice_ml_probs_test\" in globals():\n",
    "    fusion_inputs[\"voice_mlp_probs_test\"] = voice_ml_probs_test\n",
    "else:\n",
    "    print(\"âš ï¸ Voice probabilities not found â€” skipping.\")\n",
    "\n",
    "# Ground-truth labels (common reference)\n",
    "if \"y_voice_test\" in globals():\n",
    "    fusion_inputs[\"y_voice_test\"] = y_voice_test\n",
    "elif \"y_ecg_test\" in globals():\n",
    "    fusion_inputs[\"y_voice_test\"] = y_ecg_test\n",
    "else:\n",
    "    print(\"âš ï¸ No test labels found for fusion evaluation.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ðŸ’¾ Save to disk\n",
    "# ------------------------------------------------------------\n",
    "pickle.dump(fusion_inputs, open(\"fusion_inputs.pkl\", \"wb\"))\n",
    "\n",
    "print(\"\\nâœ… Fusion input probabilities saved to 'fusion_inputs.pkl'.\")\n",
    "print(\"Saved keys:\", list(fusion_inputs.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a525c9d-f278-4325-af72-7c8e722f544c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded ecg model\n",
      "âœ… Loaded text model\n",
      "âœ… Loaded face model\n",
      "âœ… Loaded voice model\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ”® FINAL LATE FUSION ENGINE â€” Moderately ECG-Focused\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pickle, os\n",
    "\n",
    "# ----------------------------\n",
    "# Load pretrained models\n",
    "# ----------------------------\n",
    "model_files = {\n",
    "    \"ecg\": \"ecg_model.pkl\",\n",
    "    \"text\": \"text_model.pkl\",\n",
    "    \"face\": \"face_model.pkl\",\n",
    "    \"voice\": \"voice_model.pkl\"\n",
    "}\n",
    "\n",
    "models = {}\n",
    "for name, path in model_files.items():\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            models[name] = pickle.load(open(path, \"rb\"))\n",
    "            print(f\"âœ… Loaded {name} model\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error loading {name} â†’ {e}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ {name} model not found â€” skipping.\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# ðŸŽ¯ Balanced Weights (ECG slightly dominant)\n",
    "# ----------------------------\n",
    "model_weights = {\n",
    "    \"ecg\": 0.40,   # ECG still leads but not too much\n",
    "    \"text\": 0.30,\n",
    "    \"face\": 0.20,\n",
    "    \"voice\": 0.10\n",
    "}\n",
    "\n",
    "# Normalize weights so total = 1\n",
    "total_w = sum(model_weights.values())\n",
    "for k in model_weights:\n",
    "    model_weights[k] /= total_w\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# ðŸš€ Late Fusion Function\n",
    "# ----------------------------\n",
    "def predict_emotion_late_fusion(fusion_inputs):\n",
    "    \"\"\"\n",
    "    fusion_inputs = {\n",
    "        'ecg': np.array([...]),\n",
    "        'text': np.array([...]),\n",
    "        'face': np.array([...]),\n",
    "        'voice': np.array([...])\n",
    "    }\n",
    "    \"\"\"\n",
    "    available_probs = []\n",
    "    used_weights = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        if name in fusion_inputs and fusion_inputs[name] is not None:\n",
    "            try:\n",
    "                p = model.predict_proba(fusion_inputs[name].reshape(1, -1))[0]\n",
    "                available_probs.append(model_weights[name] * p)\n",
    "                used_weights.append(model_weights[name])\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error predicting with {name} model: {e}\")\n",
    "\n",
    "    if not available_probs:\n",
    "        raise ValueError(\"âŒ No valid modality inputs provided for fusion.\")\n",
    "\n",
    "    final_probs = np.sum(available_probs, axis=0) / np.sum(used_weights)\n",
    "    final_class = np.argmax(final_probs)\n",
    "\n",
    "    return final_class, final_probs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "573fcc4d-2852-492a-9cb6-7dccf6f98935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded best model for ecg: best_ml_model_ecg\n",
      "âœ… Loaded text (auto-detected model: LogisticRegression)\n",
      "âœ… Loaded face (auto-detected model: XGBClassifier)\n",
      "âœ… Loaded voice (auto-detected model: XGBClassifier)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ”§ FIXED MODEL LOADING (handles multi-model dicts correctly)\n",
    "# ============================================================\n",
    "\n",
    "import pickle, os\n",
    "\n",
    "model_files = {\n",
    "    \"ecg\": \"ecg_model.pkl\",\n",
    "    \"text\": \"text_model.pkl\",\n",
    "    \"face\": \"face_model.pkl\",\n",
    "    \"voice\": \"voice_model.pkl\"\n",
    "}\n",
    "\n",
    "models = {}\n",
    "\n",
    "for name, path in model_files.items():\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            data = pickle.load(open(path, \"rb\"))\n",
    "\n",
    "            if isinstance(data, dict):\n",
    "                # 1ï¸âƒ£ Try to find best model key\n",
    "                best_keys = [k for k in data.keys() if k.startswith(\"best_ml_model\")]\n",
    "                if best_keys:\n",
    "                    models[name] = data[best_keys[0]]\n",
    "                    print(f\"âœ… Loaded best model for {name}: {best_keys[0]}\")\n",
    "                elif \"model\" in data:\n",
    "                    models[name] = data[\"model\"]\n",
    "                    print(f\"âœ… Loaded {name} from dict key 'model'\")\n",
    "                else:\n",
    "                    # 2ï¸âƒ£ Fallback: auto-detect first object with predict() method\n",
    "                    possible_models = [v for v in data.values() if hasattr(v, \"predict\")]\n",
    "                    if possible_models:\n",
    "                        models[name] = possible_models[0]\n",
    "                        print(f\"âœ… Loaded {name} (auto-detected model: {type(possible_models[0]).__name__})\")\n",
    "                    else:\n",
    "                        raise ValueError(f\"No valid model found inside {name}_model.pkl\")\n",
    "            else:\n",
    "                # Direct model object\n",
    "                models[name] = data\n",
    "                print(f\"âœ… Loaded direct model object for {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not load {name} â†’ {e}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ {name} model file not found â†’ skipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "804acbde-7c1f-43da-9c09-97403dd7aeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ECG â†’ Accuracy: 0.9027, F1: 0.8998\n",
      "âœ… TEXT â†’ Accuracy: 0.7553, F1: 0.7552\n",
      "âœ… FACE â†’ Accuracy: 0.5932, F1: 0.5912\n",
      "âœ… VOICE â†’ Accuracy: 0.7361, F1: 0.7364\n",
      "\n",
      "ðŸŽ¯ Weighted Fusion Results :\n",
      "Final Weighted Accuracy: 0.7799\n",
      "Final Weighted F1 Score: 0.7784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chira\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 16 variables whereas the saved optimizer has 30 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ðŸ”® FINAL PHASE â€” Weighted Score-Level Fusion \n",
    "# ============================================================\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# ----------------------------\n",
    "# Your model weights (importance)\n",
    "# ----------------------------\n",
    "model_weights = {\n",
    "    \"ecg\": 0.40,\n",
    "    \"text\": 0.30,\n",
    "    \"face\": 0.20,\n",
    "    \"voice\": 0.10\n",
    "}\n",
    "\n",
    "model_files = {\n",
    "    \"ecg\": \"ecg_model.pkl\",\n",
    "    \"text\": \"text_model.pkl\",\n",
    "    \"face\": \"face_model.pkl\",\n",
    "    \"voice\": \"voice_model.pkl\"\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluate each model separately\n",
    "# ----------------------------\n",
    "accuracies = {}\n",
    "f1_scores = {}\n",
    "\n",
    "for name, path in model_files.items():\n",
    "    try:\n",
    "        data = pickle.load(open(path, \"rb\"))\n",
    "        prob_keys = [k for k in data.keys() if \"probs_test\" in k]\n",
    "        y_keys = [k for k in data.keys() if \"y_\" in k and \"_test\" in k]\n",
    "\n",
    "        if prob_keys and y_keys:\n",
    "            probs = np.array(data[prob_keys[0]])\n",
    "            y_true = np.array(data[y_keys[0]])\n",
    "\n",
    "            # Handle binary or multi-class\n",
    "            if probs.ndim == 1:\n",
    "                preds = np.round(probs).astype(int)\n",
    "            else:\n",
    "                preds = np.argmax(probs, axis=1)\n",
    "\n",
    "            acc = accuracy_score(y_true, preds)\n",
    "            f1 = f1_score(y_true, preds, average='weighted')\n",
    "            accuracies[name] = acc\n",
    "            f1_scores[name] = f1\n",
    "\n",
    "            print(f\"âœ… {name.upper()} â†’ Accuracy: {acc:.4f}, F1: {f1:.4f}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ {name} missing test data, skipped.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error evaluating {name}: {e}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Weighted final fusion performance\n",
    "# ----------------------------\n",
    "final_acc = sum(model_weights[m] * accuracies.get(m, 0) for m in model_weights)\n",
    "final_f1 = sum(model_weights[m] * f1_scores.get(m, 0) for m in model_weights)\n",
    "\n",
    "print(\"\\nðŸŽ¯ Weighted Fusion Results :\")\n",
    "print(f\"Final Weighted Accuracy: {final_acc:.4f}\")\n",
    "print(f\"Final Weighted F1 Score: {final_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5acd24d-c469-45b2-970c-3c22466f88fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
